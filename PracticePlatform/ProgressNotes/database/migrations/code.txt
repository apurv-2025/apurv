# backend/migrations/versions/001_initial_migration.py
"""Initial migration

Revision ID: 001
Revises: 
Create Date: 2024-01-15 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    
    # Create users table
    op.create_table('users',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('email', sa.String(length=255), nullable=False),
        sa.Column('password_hash', sa.String(length=255), nullable=False),
        sa.Column('first_name', sa.String(length=100), nullable=False),
        sa.Column('last_name', sa.String(length=100), nullable=False),
        sa.Column('role', sa.String(length=50), nullable=False),
        sa.Column('license_number', sa.String(length=100), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('email')
    )
    
    # Create patients table
    op.create_table('patients',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('first_name', sa.String(length=100), nullable=False),
        sa.Column('last_name', sa.String(length=100), nullable=False),
        sa.Column('date_of_birth', sa.Date(), nullable=False),
        sa.Column('medical_record_number', sa.String(length=50), nullable=False),
        sa.Column('phone', sa.String(length=20), nullable=True),
        sa.Column('email', sa.String(length=255), nullable=True),
        sa.Column('address', sa.Text(), nullable=True),
        sa.Column('emergency_contact_name', sa.String(length=200), nullable=True),
        sa.Column('emergency_contact_phone', sa.String(length=20), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('medical_record_number')
    )
    
    # Create note templates table
    op.create_table('note_templates',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('name', sa.String(length=200), nullable=False),
        sa.Column('template_type', sa.String(length=50), nullable=False),
        sa.Column('structure', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column('is_system_template', sa.Boolean(), nullable=True),
        sa.Column('created_by', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.ForeignKeyConstraint(['created_by'], ['users.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create progress notes table
    op.create_table('progress_notes',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('patient_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('clinician_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('template_id', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('note_type', sa.String(length=50), nullable=False),
        sa.Column('session_date', sa.DateTime(timezone=True), nullable=False),
        sa.Column('content', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column('is_draft', sa.Boolean(), nullable=True),
        sa.Column('is_signed', sa.Boolean(), nullable=True),
        sa.Column('signed_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('signed_by', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('digital_signature', sa.Text(), nullable=True),
        sa.Column('is_locked', sa.Boolean(), nullable=True),
        sa.Column('locked_by', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('locked_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('unlock_reason', sa.Text(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.Column('version', sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(['clinician_id'], ['users.id'], ),
        sa.ForeignKeyConstraint(['locked_by'], ['users.id'], ),
        sa.ForeignKeyConstraint(['patient_id'], ['patients.id'], ),
        sa.ForeignKeyConstraint(['signed_by'], ['users.id'], ),
        sa.ForeignKeyConstraint(['template_id'], ['note_templates.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create note attachments table
    op.create_table('note_attachments',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('note_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('file_name', sa.String(length=255), nullable=False),
        sa.Column('file_path', sa.String(length=500), nullable=False),
        sa.Column('file_size', sa.Integer(), nullable=False),
        sa.Column('mime_type', sa.String(length=100), nullable=False),
        sa.Column('uploaded_by', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.ForeignKeyConstraint(['note_id'], ['progress_notes.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['uploaded_by'], ['users.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create audit logs table
    op.create_table('audit_logs',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('action', sa.String(length=100), nullable=False),
        sa.Column('resource_type', sa.String(length=50), nullable=False),
        sa.Column('resource_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('old_values', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('new_values', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('ip_address', postgresql.INET(), nullable=True),
        sa.Column('user_agent', sa.Text(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create patient clinicians table
    op.create_table('patient_clinicians',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('patient_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('clinician_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('is_primary', sa.Boolean(), nullable=True),
        sa.Column('assigned_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
        sa.ForeignKeyConstraint(['clinician_id'], ['users.id'], ),
        sa.ForeignKeyConstraint(['patient_id'], ['patients.id'], ),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('patient_id', 'clinician_id')
    )
    
    # Create indexes
    op.create_index('idx_progress_notes_patient_id', 'progress_notes', ['patient_id'])
    op.create_index('idx_progress_notes_clinician_id', 'progress_notes', ['clinician_id'])
    op.create_index('idx_progress_notes_session_date', 'progress_notes', ['session_date'])
    op.create_index('idx_progress_notes_created_at', 'progress_notes', ['created_at'])
    op.create_index('idx_audit_logs_user_id', 'audit_logs', ['user_id'])
    op.create_index('idx_audit_logs_resource_type_id', 'audit_logs', ['resource_type', 'resource_id'])
    op.create_index('idx_audit_logs_created_at', 'audit_logs', ['created_at'])
    
    # Insert default templates
    op.execute("""
        INSERT INTO note_templates (id, name, template_type, structure, is_system_template) VALUES 
        (gen_random_uuid(), 'Standard SOAP Note', 'SOAP', '{"subjective": "", "objective": "", "assessment": "", "plan": ""}', true),
        (gen_random_uuid(), 'Standard DAP Note', 'DAP', '{"data": "", "assessment": "", "plan": ""}', true),
        (gen_random_uuid(), 'Standard BIRP Note', 'BIRP', '{"behavior": "", "intervention": "", "response": "", "plan": ""}', true),
        (gen_random_uuid(), 'Standard PAIP Note', 'PAIP', '{"problem": "", "assessment": "", "intervention": "", "plan": ""}', true);
    """)
    
    # ### end Alembic commands ###

def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('patient_clinicians')
    op.drop_table('audit_logs')
    op.drop_table('note_attachments')
    op.drop_table('progress_notes')
    op.drop_table('note_templates')
    op.drop_table('patients')
    op.drop_table('users')
    # ### end Alembic commands ###

# docker-compose.prod.yml
version: '3.8'

services:
  # Production Database with backup
  db:
    image: postgres:15-alpine
    container_name: mental_health_ehr_db_prod
    environment:
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME:-mental_health_ehr}
    volumes:
      - postgres_prod_data:/var/lib/postgresql/data
      - ./backups:/backups
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Production Backend with optimizations
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    container_name: mental_health_ehr_backend_prod
    environment:
      DATABASE_URL: postgresql://${DB_USER:-postgres}:${DB_PASSWORD}@db:5432/${DB_NAME:-mental_health_ehr}
      SECRET_KEY: ${SECRET_KEY}
      CORS_ORIGINS: ${CORS_ORIGINS}
      ENVIRONMENT: production
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - backend_uploads:/app/uploads
      - ./logs:/app/logs
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # Production Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    container_name: mental_health_ehr_frontend_prod
    depends_on:
      - backend
    networks:
      - app-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    container_name: mental_health_ehr_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - frontend
      - backend
    networks:
      - app-network
    restart: unless-stopped

  # Redis for caching (optional)
  redis:
    image: redis:7-alpine
    container_name: mental_health_ehr_redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - app-network
    restart: unless-stopped
    profiles:
      - cache

volumes:
  postgres_prod_data:
  backend_uploads:
  redis_data:

networks:
  app-network:
    driver: bridge

# backend/Dockerfile.prod
FROM python:3.11-slim as builder

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libpq-dev \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Production stage
FROM python:3.11-slim

WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    libpq-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy Python dependencies from builder stage
COPY --from=builder /root/.local /root/.local

# Copy application code
COPY . .

# Create non-root user
RUN useradd --create-home --shell /bin/bash app
RUN chown -R app:app /app
USER app

# Create uploads and logs directories
RUN mkdir -p uploads logs

# Make sure scripts are executable
RUN chmod +x scripts/*.sh

# Add local bin to PATH
ENV PATH=/root/.local/bin:$PATH

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Command to run the application
CMD ["gunicorn", "main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]

# frontend/Dockerfile.prod
# Build stage
FROM node:18-alpine as builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy source code
COPY . .

# Build the application
RUN npm run build

# Production stage
FROM nginx:alpine

# Copy built app from builder stage
COPY --from=builder /app/dist /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Add non-root user
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001

# Change ownership of nginx directories
RUN chown -R nextjs:nodejs /usr/share/nginx/html
RUN chown -R nextjs:nodejs /var/cache/nginx
RUN chown -R nextjs:nodejs /var/log/nginx
RUN chown -R nextjs:nodejs /etc/nginx/conf.d
RUN touch /var/run/nginx.pid
RUN chown -R nextjs:nodejs /var/run/nginx.pid

# Switch to non-root user
USER nextjs

# Expose port
EXPOSE 80

# Start nginx
CMD ["nginx", "-g", "daemon off;"]

# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }

    upstream frontend {
        server frontend:80;
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;

    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private must-revalidate no_last_modified no_etag auth;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/x-javascript
        application/xml+rss
        application/javascript
        application/json;

    server {
        listen 80;
        server_name localhost;

        # Frontend
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Backend API
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://backend/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # CORS headers
            add_header Access-Control-Allow-Origin $http_origin;
            add_header Access-Control-Allow-Methods "GET, POST, PUT, DELETE, OPTIONS";
            add_header Access-Control-Allow-Headers "Origin, X-Requested-With, Content-Type, Accept, Authorization";
            add_header Access-Control-Allow-Credentials true;
            
            if ($request_method = 'OPTIONS') {
                return 204;
            }
        }

        # Special rate limiting for login endpoint
        location /api/auth/login {
            limit_req zone=login burst=3 nodelay;
            
            proxy_pass http://backend/auth/login;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Health check endpoint
        location /health {
            proxy_pass http://backend/health;
            access_log off;
        }
    }

    # HTTPS configuration (uncomment for production with SSL)
    # server {
    #     listen 443 ssl http2;
    #     server_name your-domain.com;
    #
    #     ssl_certificate /etc/nginx/ssl/cert.pem;
    #     ssl_certificate_key /etc/nginx/ssl/key.pem;
    #     ssl_protocols TLSv1.2 TLSv1.3;
    #     ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
    #     ssl_prefer_server_ciphers off;
    #
    #     # Rest of configuration same as HTTP server
    # }
}

# scripts/deploy.sh
#!/bin/bash

# Production deployment script

set -e

echo "ğŸš€ Starting production deployment..."

# Check if required environment variables are set
if [ -z "$SECRET_KEY" ]; then
    echo "âŒ SECRET_KEY environment variable is required"
    exit 1
fi

if [ -z "$DB_PASSWORD" ]; then
    echo "âŒ DB_PASSWORD environment variable is required"
    exit 1
fi

# Pull latest changes
echo "ğŸ“¥ Pulling latest changes..."
git pull origin main

# Build and start services
echo "ğŸ—ï¸ Building and starting services..."
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build

# Wait for database to be ready
echo "â³ Waiting for database to be ready..."
sleep 10

# Run database migrations
echo "ğŸ—ƒï¸ Running database migrations..."
docker-compose exec backend alembic upgrade head

# Create default admin user if it doesn't exist
echo "ğŸ‘¤ Creating default admin user..."
docker-compose exec backend python -c "
from sqlalchemy.orm import Session
from database import SessionLocal
from models import User
from auth import get_password_hash

db = SessionLocal()
existing_user = db.query(User).filter(User.email == 'admin@clinic.com').first()

if not existing_user:
    admin_user = User(
        email='admin@clinic.com',
        password_hash=get_password_hash('admin123'),
        first_name='Admin',
        last_name='User',
        role='admin'
    )
    db.add(admin_user)
    db.commit()
    print('Admin user created successfully')
else:
    print('Admin user already exists')

db.close()
"

# Health check
echo "ğŸ¥ Performing health check..."
sleep 5
if curl -f http://localhost/health > /dev/null 2>&1; then
    echo "âœ… Application is healthy and running!"
else
    echo "âŒ Health check failed"
    exit 1
fi

echo "ğŸ‰ Deployment completed successfully!"
echo "ğŸ“ Application is available at: http://localhost"
echo "ğŸ“š API documentation: http://localhost/api/docs"

# scripts/backup.sh
#!/bin/bash

# Database backup script

set -e

BACKUP_DIR="./backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/backup_${TIMESTAMP}.sql"

echo "ğŸ—ƒï¸ Starting database backup..."

# Create backup directory if it doesn't exist
mkdir -p $BACKUP_DIR

# Create database backup
docker-compose exec -T db pg_dump -U postgres mental_health_ehr > $BACKUP_FILE

# Compress the backup
gzip $BACKUP_FILE

echo "âœ… Database backup completed: ${BACKUP_FILE}.gz"

# Keep only last 30 backups
find $BACKUP_DIR -name "backup_*.sql.gz" -type f -mtime +30 -delete

echo "ğŸ§¹ Old backups cleaned up"

# scripts/restore.sh
#!/bin/bash

# Database restore script

if [ -z "$1" ]; then
    echo "Usage: $0 <backup_file>"
    echo "Available backups:"
    ls -la ./backups/backup_*.sql.gz
    exit 1
fi

BACKUP_FILE=$1

echo "ğŸ”„ Starting database restore from $BACKUP_FILE..."

# Check if backup file exists
if [ ! -f "$BACKUP_FILE" ]; then
    echo "âŒ Backup file not found: $BACKUP_FILE"
    exit 1
fi

# Decompress if gzipped
if [[ $BACKUP_FILE == *.gz ]]; then
    echo "ğŸ“¦ Decompressing backup file..."
    gunzip -c $BACKUP_FILE > temp_restore.sql
    BACKUP_FILE="temp_restore.sql"
fi

# Stop application
echo "â¹ï¸ Stopping application..."
docker-compose down

# Start only database
echo "ğŸ—ƒï¸ Starting database..."
docker-compose up -d db

# Wait for database to be ready
echo "â³ Waiting for database to be ready..."
sleep 10

# Drop existing database and recreate
echo "ğŸ—‘ï¸ Recreating database..."
docker-compose exec db psql -U postgres -c "DROP DATABASE IF EXISTS mental_health_ehr;"
docker-compose exec db psql -U postgres -c "CREATE DATABASE mental_health_ehr;"

# Restore backup
echo "ğŸ“¥ Restoring database..."
docker-compose exec -T db psql -U postgres mental_health_ehr < $BACKUP_FILE

# Clean up temporary file
if [ -f "temp_restore.sql" ]; then
    rm temp_restore.sql
fi

# Start full application
echo "ğŸš€ Starting application..."
docker-compose up -d

echo "âœ… Database restore completed!"

# Makefile
.PHONY: help install dev test build deploy clean backup restore

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-15s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

install: ## Install dependencies
	@echo "Installing backend dependencies..."
	cd backend && pip install -r requirements.txt
	@echo "Installing frontend dependencies..."
	cd frontend && npm install

dev: ## Start development environment
	@echo "Starting development environment..."
	docker-compose up -d

test: ## Run tests
	@echo "Running backend tests..."
	cd backend && pytest
	@echo "Running frontend tests..."
	cd frontend && npm test

build: ## Build production images
	@echo "Building production images..."
	docker-compose -f docker-compose.yml -f docker-compose.prod.yml build

deploy: ## Deploy to production
	@echo "Deploying to production..."
	./scripts/deploy.sh

clean: ## Clean up containers and volumes
	@echo "Cleaning up..."
	docker-compose down -v
	docker system prune -f

backup: ## Create database backup
	@echo "Creating database backup..."
	./scripts/backup.sh

restore: ## Restore database from backup
	@echo "Available backups:"
	@ls -la ./backups/backup_*.sql.gz
	@read -p "Enter backup filename: " backup; ./scripts/restore.sh $$backup

logs: ## Show application logs
	docker-compose logs -f

health: ## Check application health
	@echo "Checking application health..."
	@curl -f http://localhost/health && echo "âœ… Application is healthy" || echo "âŒ Application is unhealthy"
